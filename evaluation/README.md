# Evaluation & Testing

This folder contains evaluation datasets and testing scripts to verify system accuracy.

## ğŸ“ Structure

```
evaluation/
â”œâ”€â”€ test_cases/              # Test cases with expected outputs
â”‚   â”œâ”€â”€ ontario_tickets.json
â”‚   â””â”€â”€ california_tickets.json
â”œâ”€â”€ scripts/                 # Evaluation scripts
â”‚   â”œâ”€â”€ run_evaluation.py
â”‚   â””â”€â”€ compare_results.py
â””â”€â”€ results/                 # Evaluation results
    â””â”€â”€ latest_results.json
```

## ğŸ¯ Test Case Format

Each test case includes:
- Input: Ticket data + question
- Expected: Key points that should appear in answer
- Metrics: What to check (offence identified, demerit points, options presented)

## ğŸ“Š Evaluation Metrics

1. **Offence Identification**: Did it correctly identify the offence?
2. **Demerit Points**: Were points correctly stated?
3. **Options Presented**: Were FIGHT and PAY options both provided?
4. **Accuracy**: Were legal facts correct?
5. **Completeness**: Were all important points covered?

## ğŸš€ Running Evaluation

```bash
cd evaluation/scripts
python run_evaluation.py
```

This will:
1. Load test cases
2. Run queries through the system
3. Compare outputs to expected results
4. Generate a report

