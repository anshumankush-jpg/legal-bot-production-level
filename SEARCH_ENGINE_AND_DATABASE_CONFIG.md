# üîç Search Engine & Database Configuration

## Current Configuration

### **Search Engine (Embeddings):**
- **Provider:** Sentence Transformers
- **Model:** `all-MiniLM-L6-v2`
- **Dimensions:** 384
- **Type:** Local (runs on your computer)
- **Cost:** FREE
- **Status:** ‚úÖ Active

### **Vector Database:**
- **Type:** FAISS (Facebook AI Similarity Search)
- **Storage:** Local file-based
- **Location:** `backend/data/faiss/`
- **Files:**
  - `index.faiss` - Vector index
  - `metadata.jsonl` - Document metadata
- **Status:** ‚úÖ Active

---

## How It Works

### **1. Document Ingestion:**
```
Document (PDF/HTML/JSON)
    ‚Üì
Text Extraction
    ‚Üì
Chunking (Split into pieces)
    ‚Üì
Sentence Transformers (Local)
    ‚Üí Converts text to 384-dim vectors
    ‚Üì
FAISS Index (Local Storage)
    ‚Üí Stores vectors + metadata
```

### **2. Query Processing:**
```
User Question
    ‚Üì
Sentence Transformers (Local)
    ‚Üí Converts question to 384-dim vector
    ‚Üì
FAISS Search (Local)
    ‚Üí Finds similar document chunks
    ‚Üí Returns top-K results
    ‚Üì
Retrieved Context
    ‚Üì
OpenAI GPT-4o (API)
    ‚Üí Generates answer from context
    ‚Üì
Response to User
```

---

## Configuration Files

### **`backend/app/core/config.py`:**
```python
# Embedding Provider - Sentence Transformers (local, free)
EMBEDDING_PROVIDER: str = "sentence_transformers"
SENTENCE_TRANSFORMER_MODEL: str = "all-MiniLM-L6-v2"

# FAISS Configuration (local vector database)
FAISS_INDEX_PATH: str = "./data/faiss/index.faiss"
FAISS_METADATA_PATH: str = "./data/faiss/metadata.jsonl"
```

### **`backend/app/vector_store/__init__.py`:**
- Automatically selects FAISS (Azure is disabled)
- Creates FAISS store with correct dimensions

### **`backend/app/embeddings/embedding_service.py`:**
- Initializes Sentence Transformer model on startup
- Auto-detects embedding dimensions (384 for all-MiniLM-L6-v2)

---

## Benefits

### **Sentence Transformers:**
- ‚úÖ FREE (no API costs)
- ‚úÖ Works offline
- ‚úÖ Fast local processing
- ‚úÖ No rate limits
- ‚úÖ Good quality for document search

### **FAISS:**
- ‚úÖ FREE (local file storage)
- ‚úÖ Fast similarity search
- ‚úÖ No cloud dependencies
- ‚úÖ Works offline
- ‚úÖ Scales to millions of vectors

---

## Verification

### Check Configuration:
```python
from app.core.config import settings
print("Embedding Provider:", settings.EMBEDDING_PROVIDER)
print("Model:", settings.SENTENCE_TRANSFORMER_MODEL)
print("Dimensions:", settings.EMBEDDING_DIMENSIONS)
print("FAISS Index:", settings.FAISS_INDEX_PATH)
```

### Check Active Services:
```python
from app.embeddings.embedding_service import get_embedding_service
from app.vector_store import get_vector_store

embedding_service = get_embedding_service()
vector_store = get_vector_store()

print("Embedding Service:", type(embedding_service).__name__)
print("Vector Store:", type(vector_store).__name__)
```

---

## Summary

‚úÖ **Search Engine:** Sentence Transformers (local, free)  
‚úÖ **Database:** FAISS (local file storage)  
‚úÖ **Status:** Both active and configured  
‚úÖ **Cost:** FREE (no cloud services)  
‚úÖ **Offline:** Works without internet (except for OpenAI chat)

**Everything runs locally except for the final answer generation (OpenAI API)!** üöÄ

